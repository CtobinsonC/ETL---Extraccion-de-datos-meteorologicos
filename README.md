# üå¶Ô∏è End-to-End Weather ETL Pipeline

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![Airflow](https://img.shields.io/badge/Orchestration-Apache%20Airflow-red.svg)](https://airflow.apache.org/)
[![PostgreSQL](https://img.shields.io/badge/Database-PostgreSQL-blue.svg)](https://www.postgresql.org/)
[![Streamlit](https://img.shields.io/badge/Visualization-Streamlit-FF4B4B.svg)](https://streamlit.io/)

Este proyecto implementa un pipeline de datos (ETL) completo y profesional. Automatiza la extracci√≥n de datos meteorol√≥gicos desde una API p√∫blica, su limpieza y normalizaci√≥n con Pandas, y su carga incremental en un almac√©n de datos PostgreSQL controlado por Airflow.

![Dashboard Preview](dashboard.png)
*Vista previa del dashboard interactivo generado con Streamlit.*

## üöÄ Arquitectura del Sistema

El flujo de datos sigue un dise√±o modular para garantizar la escalabilidad y mantenibilidad:

```mermaid
graph LR
    subgraph "External Sources"
        API["Open-Meteo API"]
    end

    subgraph "ETL Engine (Python)"
        EX["Extraction<br/>(Requests + Retries)"]
        TR["Transformation<br/>(Pandas Cleaning)"]
        LO["Loading<br/>(PostgreSQL Upsert)"]
    end

    subgraph "Orchestration"
        AF["Apache Airflow<br/>(Dockerized)"]
    end

    subgraph "Consumer Layer"
        DB[("PostgreSQL")]
        ST["Streamlit App<br/>(Dashboard)"]
    end

    API --> EX
    EX --> TR
    TR --> LO
    LO --> DB
    DB --> ST
    AF -.-> EX
    AF -.-> TR
    AF -.-> LO
```

## üìÇ Estructura del Proyecto

*   `dags/`: Definici√≥n de flujos de trabajo de Airflow (DAGs).
*   `src/`: M√≥dulos principales del motor ETL (Extract, Transform, Load).
*   `app_viz.py`: Aplicaci√≥n de visualizaci√≥n interactiva.
*   `docker-compose.yml`: Orquestaci√≥n de contenedores (Airflow + Postgres).
*   `test_local.py`: Runner de pruebas unitarias para l√≥gica de negocio.

## üõ†Ô∏è Stack Tecnol√≥gico

- **Lenguaje:** Python 3.x
- **Librer√≠as ETL:** Pandas, Requests, SQLAlchemy
- **Base de Datos:** PostgreSQL 13
- **Orquestaci√≥n:** Apache Airflow 2.7.1
- **Infraestructura:** Docker & Docker Compose
- **Visualizaci√≥n:** Streamlit & Plotly

## ‚öôÔ∏è Configuraci√≥n y Ejecuci√≥n

### 1. Requisitos
- Docker y Docker Desktop
- Python 3.9+ (para ejecuci√≥n local del dashboard)

### 2. Despliegue de Infraestructura
Levanta el ecosistema de Airflow y PostgreSQL:
```bash
docker-compose up -d
```
*El sistema inicializar√° autom√°ticamente la base de datos y crear√° el usuario admin (`airflow`/`airflow`).*

### 3. Ejecuci√≥n del Pipeline
1. Accede a Airflow en `http://localhost:8080`.
2. Activa y dispara el DAG `weather_etl_dag`.
3. Verifica los logs para asegurar que la carga incremental (Upsert) ha sido exitosa.

### 4. Visualizaci√≥n de Datos
Ejecuta el dashboard interactivo:
```bash
python -m streamlit run app_viz.py
```

## ‚ú® Buenas Pr√°cticas Implementadas
- **Modularidad:** Separaci√≥n de preocupaciones en capas E, T y L.
- **Resiliencia:** Reintentos autom√°ticos con *Exponential Backoff* en la extracci√≥n.
- **Eficiencia:** Carga incremental mediante l√≥gica **Upsert** (ON CONFLICT) para evitar duplicados.
- **Observabilidad:** Logs estructurados en cada etapa del proceso.
